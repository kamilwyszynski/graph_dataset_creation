{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "creating_graph_dataset",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "16WWxmFJIhbKB-dpCrS0Jh7DqI3VHyb4M",
      "authorship_tag": "ABX9TyO3VrOkb8IeGoBpTH53Thye",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamilwyszynski/graph_dataset_creation/blob/main/creating_graph_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIR9p81odbKw"
      },
      "source": [
        "# Creating a custom graph dataset\n",
        "<br>\n",
        "\n",
        "*This notebook demostrates the creation of a custom PyTorch Geometric graph dataset*\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        " **Contents**\n",
        "\n",
        "\n",
        "\n",
        "1.   Getting data in the correct shape\n",
        "2.   Creating a Dataset class\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0Z4cA4cGk-w"
      },
      "source": [
        "# Installing Pytorch Geometric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL4QFEZAGkob",
        "outputId": "2598cc39-911f-45a0-f901-dcd76f6e3cba"
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install torch-geometric\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "!pip install torch_sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
        "!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/47/58b9f3e6f611dfd17fb8bd9ed3e6f93b7ee662fb85bdfee3565e8979ddf7/pip-21.0-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 7.6MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-21.0\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-1.6.3.tar.gz (186 kB)\n",
            "\u001b[K     |████████████████████████████████| 186 kB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.5)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.15)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.51.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.1.5)\n",
            "Collecting rdflib\n",
            "  Downloading rdflib-5.0.0-py3-none-any.whl (231 kB)\n",
            "\u001b[K     |████████████████████████████████| 231 kB 14.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
            "Collecting ase\n",
            "  Downloading ase-3.21.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 16.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.11.2)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ase->torch-geometric) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=2.0.0->ase->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->torch-geometric) (1.1.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (51.3.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Collecting isodate\n",
            "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (1.0.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.16.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-1.6.3-py3-none-any.whl size=322718 sha256=e6248e19e157b12295d211f509ee020cb53e1ec1f28b48db57d54ca1efb0524a\n",
            "  Stored in directory: /root/.cache/pip/wheels/55/be/05/a068e58b901163f619e69824868d3ca17171b1482446f585d8\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, rdflib, ase, torch-geometric\n",
            "Successfully installed ase-3.21.1 isodate-0.6.0 rdflib-5.0.0 torch-geometric-1.6.3\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://pytorch-geometric.com/whl/torch-1.7.0%2Bcu101/torch_scatter-2.0.5-cp36-cp36m-linux_x86_64.whl (11.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.9 MB 6.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.5\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
            "Collecting torch_sparse\n",
            "  Downloading https://pytorch-geometric.com/whl/torch-1.7.0%2Bcu101/torch_sparse-0.6.8-cp36-cp36m-linux_x86_64.whl (24.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.3 MB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch_sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch_sparse) (1.19.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.8\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0+cu101.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://pytorch-geometric.com/whl/torch-1.7.0%2Bcu101/torch_cluster-1.5.8-cp36-cp36m-linux_x86_64.whl (21.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.5 MB 90.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.5.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfn6tq_ig5r5"
      },
      "source": [
        "# Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6Om7jc3g8eJ"
      },
      "source": [
        "# word embedding\n",
        "import gensim.downloader as gensim_api\n",
        "# grapth dataset\n",
        "import torch\n",
        "from torch_geometric.data import InMemoryDataset, Data\n",
        "# combinations\n",
        "import itertools"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nnbxRvpFYqR"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iacwVbPpgz6E",
        "outputId": "f8bab393-81c5-4887-b7aa-d0d079096fff"
      },
      "source": [
        "nlp = gensim_api.load(\"glove-wiki-gigaword-300\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[===============================================---] 94.9% 356.8/376.1MB downloaded"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZRtkAl5Cuv7",
        "outputId": "a09cfbad-6739-4e5a-a3e4-64ab6205f8ba"
      },
      "source": [
        "nlp"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7f0c398c3668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLPfH8zdy4r1",
        "outputId": "e0c06909-bc69-4462-c52d-7ab42b1e2677"
      },
      "source": [
        "nlp.most_similar(['whale'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('whales', 0.78084397315979),\n",
              " ('humpback', 0.6860450506210327),\n",
              " ('shark', 0.6499471664428711),\n",
              " ('dolphin', 0.6322990655899048),\n",
              " ('minke', 0.5734179019927979),\n",
              " ('whaling', 0.5654325485229492),\n",
              " ('orca', 0.5450523495674133),\n",
              " ('fish', 0.5387887954711914),\n",
              " ('tuna', 0.494992733001709),\n",
              " ('hunts', 0.49030470848083496)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsBSuvFQ1q8p",
        "outputId": "0df8950a-45fb-49ae-d2c0-46ee1ab02858"
      },
      "source": [
        "w1, w2 = 'whale', 'fish'\n",
        "\n",
        "print(nlp.similar_by_word(w1))\n",
        "print(nlp.distance(w1, w2))\n",
        "print(nlp.n_similarity(w1, w2))\n",
        "print(nlp.similarity(w1,w2)) # Use this"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('whales', 0.78084397315979), ('humpback', 0.6860450506210327), ('shark', 0.6499471664428711), ('dolphin', 0.6322990655899048), ('minke', 0.5734179019927979), ('whaling', 0.5654325485229492), ('orca', 0.5450523495674133), ('fish', 0.5387887954711914), ('tuna', 0.494992733001709), ('hunts', 0.49030470848083496)]\n",
            "0.46121108531951904\n",
            "0.7872955\n",
            "0.5387889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU_HHKcbgq6C"
      },
      "source": [
        "### Selecting Datapoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9Urj7elPqpb"
      },
      "source": [
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKyILH2EP_ww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca5f876a-9343-44ac-9128-fde3897d3480"
      },
      "source": [
        "# read cifar100 metadata to retrieve the words\n",
        "cifar_path = r'/content/drive/MyDrive/Colab Notebooks/Word2Vec/cifar-100-python/meta'\n",
        "cifar_dict = unpickle(cifar_path)\n",
        "cifar_dict.keys()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys([b'fine_label_names', b'coarse_label_names'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D4Hem2xFA5X"
      },
      "source": [
        "# picking only the words that exist in the nlp object\n",
        "\n",
        "def is_in_nlp(word):\n",
        "    if word in nlp.vocab.keys():\n",
        "        return True\n",
        "    else:\n",
        "        return False\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cySlrbrzRffM"
      },
      "source": [
        "words = [i.decode(\"utf-8\") for i in cifar_dict[b'fine_label_names'] if is_in_nlp(i.decode(\"utf-8\"))]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG_GWb779OOg"
      },
      "source": [
        "### Node feature matrix with shape\n",
        "\n",
        "<br>\n",
        "\n",
        "In this case, we're creating a graph of words and the distances between them.\n",
        "Therefore, for the feature matrix, we will use sparse matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gymcrVyTVnq8"
      },
      "source": [
        "x = torch.eye(len(words))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJPLIHfDBbs6"
      },
      "source": [
        "### Graph connectivity \n",
        "\n",
        "The word graph should be fully connected.\n",
        "I didn't find documentation about automating the full connectivity of a graph.\n",
        "Therefore, I need to generate one myself.\n",
        "\n",
        "<br>\n",
        "\n",
        "This connectivity array should be of a shape [ 2, number_of_nodes ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt0jyweuBt5l",
        "outputId": "34a12434-c924-45b4-ce1a-4e68574b3e6b"
      },
      "source": [
        "torch.tensor([[1,2,3],[3,4,5]]).shape"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKi4_kyWAB6-"
      },
      "source": [
        "edge_index = torch.tensor(list(itertools.combinations(range(len(words)), 2)))\n",
        "\n",
        "edge_index = torch.reshape(edge_index, (edge_index.shape[1], edge_index.shape[0]))"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dZOYH1EDui1"
      },
      "source": [
        "###  Edge feature matrix\n",
        "\n",
        "This is a tensor that contains the features of an edge.\n",
        "In this case, each edge will hold one value - the similarity between the two words it's connecting.\n",
        "\n",
        "<br>\n",
        "\n",
        "It should be of size [ num_edges, num_edge_features ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkQU31_PDnWv",
        "outputId": "de9fbfc9-3231-47e3-a89d-89f61f398c04"
      },
      "source": [
        "word_similarities = [nlp.similarity(w1,w2) for w1,w2 in list(itertools.combinations(words, 2))]\n",
        "\n",
        "edge_attr = torch.tensor(word_similarities)\n",
        "edge_attr"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1858, 0.2127, 0.1040,  ..., 0.1771, 0.1391, 0.0273])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCzE4_sYgupH"
      },
      "source": [
        "# PyTorch Geometric Dataset Implementation\n",
        "\n",
        "Here, I'm going to repeat the previously taken steps in the dataset class itself in order to increase reusability of it.\n",
        "\n",
        "<br>\n",
        "\n",
        "The Data object will be created from the data components constructed above in this notebook.\n",
        "Then the `torch_geometric.data.InMemoryDataset.collate()` function is called to optimise the memory used by the dataset and to convert the tensors into a usable `InMemoryDataset`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R21Si06bdVbr"
      },
      "source": [
        "class CIFARGraph(InMemoryDataset):\n",
        "    r\"\"\"Network of almost 100 words from the CIFAR100 dataset and their \n",
        "        similarity to each other.\n",
        "\n",
        "    Args:\n",
        "        word2vec (callable): A trained word2vec model. \n",
        "            Instance of gensim.models.keyedvectors.Word2VecKeyedVectors class.\n",
        "        word_list (callable): A list of words needed in the dataset.\n",
        "        transform (callable, optional): A function/transform that takes in an\n",
        "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
        "            version. The data object will be transformed before every access.\n",
        "            (default: :obj:`None`)\n",
        "    \"\"\"\n",
        "    def __init__(self, word2vec, word_list, transform=None):\n",
        "        super(CIFARGraph, self).__init__('.', transform, None, None)\n",
        "\n",
        "        word_list = [i for i in word_list if self.is_in_nlp(i)]\n",
        "\n",
        "        x = torch.eye(len(words), dtype=torch.float)\n",
        "\n",
        "        node_combinations = torch.tensor(list(itertools.combinations(range(len(word_list)), 2)))\n",
        "        edge_index = torch.reshape(node_combinations, (2, node_combinations.shape[0]))\n",
        "\n",
        "        word_similarities = [word2vec.similarity(w1,w2) for w1,w2 in list(itertools.combinations(word_list, 2))]\n",
        "        edge_attr = torch.tensor(word_similarities)\n",
        "\n",
        "        # y = torch.tensor(word_list)\n",
        "\n",
        "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
        "\n",
        "        self.data, self.slices = self.collate([data])\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}()'.format(self.__class__.__name__)\n",
        "    \n",
        "    def is_in_nlp(self, word):\n",
        "        if word in word2vec.vocab.keys():\n",
        "            return True\n",
        "        else:\n",
        "            return False\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l06thfEfG-jg"
      },
      "source": [
        "# Creating an instance of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT9sHZrwK5E6"
      },
      "source": [
        "word_list = [i.decode(\"utf-8\") for i in cifar_dict[b'fine_label_names']]\n",
        "word2vec = gensim_api.load(\"glove-wiki-gigaword-300\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtMwY6aMHA3W"
      },
      "source": [
        "cifar = CIFARGraph(word2vec, word_list)"
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}